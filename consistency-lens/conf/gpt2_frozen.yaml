# Configuration for GPT-2 Frozen Base Model Experiment
# This config trains consistency lens on GPT-2 with the base model frozen throughout

defaults:
  - config
  - _self_

# Model parameters
model_name: "openai-community/gpt2"  # GPT-2 124M parameters
tokenizer_name: "openai-community/gpt2"
layer_l: 6                            # Middle layer (GPT-2 has 12 layers, 0-indexed)

# Decoder parameters
decoder_prompt: "<|endoftext|>Short explanation of <embed>. Language, topic, sentiment, claims, speaker, style, etc:"
encoder_prompt: "<|endoftext|>Summary of the above text, encapsulating the main points and message, style and details:"
lm_loss_natural_prefix: "<|endoftext|>Short explanation of something. Language, topic, sentiment, claims, speaker, style, etc:"  

# Training hyperparameters
t_text: 10                            # Number of tokens to generate with the Decoder
learning_rate: 1.0e-3                # Good starting point for transformers
batch_size: 128                     # Can go higher with 8xH100

# Learning rate scheduler
lr_scheduler:
  type: "cosine_anneal"                      # Cosine annealing works well
  warmup_steps: "64os"                # 64 optimizer steps = ~1024 micro-steps
  warmup_start_factor: 0.1            # Start at 10% of target LR
  eta_min: 1.0e-5                     # Min LR at end of cosine
  linear_decay_steps: "200os"         # 200 optimizer steps = ~3200 micro-steps
  initial_high_lr: 1.0e-2

# Training duration
num_train_epochs: 2                   # 2 epochs should be plenty for OpenWebText
max_train_steps: 0                    # Use epochs instead

compile_models: true                  # Enable torch.compile for speed
log_interval: "6os"                   # Log every 6 optimizer steps = ~100 micro-steps
wandb_log_interval: "1os"             # WandB every optimizer step = ~16 micro-steps
grad_clip: 1.0

# Trainable components - FROZEN base model
trainable_components:
  decoder:
    base_model: false                 # FROZEN throughout
    projection_layer: true            # Train projection
    eye_init: true
    output_head: false                # Keep LM head frozen too
  encoder:
    base_model: false                 # FROZEN throughout
    use_base_model: true
    projection_layer: true            # Train projection
    eye_init: true

# No freeze schedule needed since we're keeping it frozen
freeze_schedule:
  enabled: false

# Custom learning rates
custom_lr_multipliers:
  projection_layers: 1.0              # Same LR for projections

# Gumbel temperature schedule
gumbel_tau_schedule:
  type: "linear_decay_after_constant"
  constant_steps_before_decay: "625os"  # 625 optimizer steps = ~10000 micro-steps
  start_value: 1.0
  end_value: 0.1
  num_steps: -1                       # Use total training steps

# LM weight schedule (alpha)
# Ramps up language modeling constraint to gradually introduce linguistic fluency
# while KL loss (functional preservation) remains the constant fundamental objective  
alpha_schedule:
  type: "linear_warmup"
  start_value: 0.0
  end_value: 1
  num_steps: "312os"                  # 312 optimizer steps = ~5000 micro-steps

# Activation dumper config for OpenWebText
activation_dumper:
  num_samples: -1               # 5M samples from OpenWebText
  seq_len: 128                        # Longer sequences for GPT-2
  use_hf_dataset: true
  hf_dataset_name: "openwebtext"      # OpenWebText dataset
  hf_split: "train"
  dataset_cache_dir: "./data/corpus/openwebtext"
  output_dir: "./data/openwebtext_train"
  batch_size:  256                    # Large batch for dumping
  # Validation split
  val_hf_split: "train"               # OpenWebText only has train split
  val_output_dir: "./data/openwebtext_val"
  val_num_samples: 50000              # 50k validation samples

# Validation settings
val_activation_dir: "./data/openwebtext_val"
val_fraction: 0.01                    # Only used if separate val dir doesn't exist
split_seed: 42
val_interval: "31os"                  # Validate every 31 optimizer steps = ~500 micro-steps

# WandB configuration
wandb:
  project: "consistency-lens-gpt2"
  mode: "online"

# Checkpoint configuration
checkpoint:
  enabled: true
  output_dir: "outputs/checkpoints"
  name_pattern: "gpt2_frozen_step{step}_epoch{epoch}"
  save_every_n_steps: "312os"        # Save every 312 optimizer steps = ~5000 micro-steps
  save_every_n_epochs: 1
  save_at_end: true
  track_best_n: 3
  best_metric: "val_loss"
  best_mode: "min"
  save_components:
    models: true
    optimizer: true
    scheduler: true
    config: true
    metrics: true
  max_checkpoints: 10
  delete_old_checkpoints: true

# Loss weights
lm_base_weight: 1                  # Base LM weight; final LM multiplier = lm_base_weight * alpha_schedule
kl_base_weight: 1.0
entropy_weight: 0.000                 # Small entropy bonus

# Verbose samples for monitoring
verbose_samples:
  enabled: true
  num_samples: 5
  interval_type: "steps"
  interval: "62os"                    # Every 62 optimizer steps = ~1000 micro-steps
  top_n_predictions: 5
  generate_continuation: true
  continuation_tokens: 50

# Gradient accumulation (assuming ~16 steps)
gradient_accumulation_steps: 16       # Adjust based on GPU memory

# For multi-GPU: adjust per_device_batch_size in DeepSpeed config
# Total batch_size = per_device_batch_size * num_gpus * gradient_accumulation_steps