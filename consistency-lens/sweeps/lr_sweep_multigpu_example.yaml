# Example W&B sweep configuration with multi-GPU support
program: scripts/wandb_sweep_train_only.py
method: grid
metric:
  name: loss/kl
  goal: minimize

parameters:
  config:
    value: conf/gpt2_frozen_postfix.yaml
  
  # Learning rate sweep
  learning_rate:
    values: [3e-3, 1e-3, 5e-3]
  
  # Batch size options (per GPU)
  batch_size:
    values: [256]
  
  # Number of GPUs to use (1-8)
  num_gpus:
    values: [1]

  gradient_accumulation_steps:
    values: [16,32,64]


  gumbel_tau_schedule.start_value:
    values: [2.0, 3.0, 5.0]

  num_train_epochs:
    value: 10


  early_termination.enabled:
    value: true

  early_termination.steps:
    value: "5000s"
  
  # Run suffix for unique naming
  run_suffix:
    value: "_sweep_multigpu_"
  
  # # Other fixed parameters
  # max_train_steps:
  #   value: 10000
  
  # gradient_accumulation_steps:
  #   value: 4
  
  # # Gumbel temperature schedule
  # gumbel_tau_schedule:
  #   value: "2.0->0.5@linear"
  
  # # Alpha schedule for LM loss
  # alpha_schedule:
  #   value: "0.0->1.0@linear"

# Optional: Add early termination
early_terminate:
  type: hyperband
  s: 2
  eta: 3
  max_iter: 27