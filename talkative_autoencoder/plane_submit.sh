
sleep 600
scripts/submit_with_config.sh config=conf/gemma2_WILDCHAT_9b_frozen_nopostfix.yaml run_suffix=NO_ENC_WILDCHAT_DIR_VARY_2 num_gpus_train=1 num_train_epochs=2 t_text=8 batch_size=16 group_n=8 trainable_components.encoder.output_layer=30 layer_l=30 gradient_accumulation_steps=1 gumbel_tau_schedule.constant_steps_before_decay=128os lm_base_weight=0 mse_weight=1.0 GRPO_weight=1.0 kl_base_weight=0 learning_rate=3e-4 weight_decay=0.00001 trainable_components.decoder.use_kv_cache=true lr_scheduler.warmup_steps=1000 freeze_schedule.enabled=false custom_lr_multipliers.overall_encoder=0.2 trainable_components.encoder.add_current_token=true trainable_components.decoder.projection_init_method=scale entropy_weight=0.5 verbose_samples.num_samples=2 verbose_samples.interval=2000 dataset.on_the_fly.enabled=true dataset.on_the_fly.samples_per_regeneration_cycle=1000 dataset.on_the_fly.max_val_samples=3000 nice=20000 trainable_components.decoder.patch_all_layers=2 resume_reset_steps=true wandb_resume_id=false grad_clip_enc=1 grad_clip=1 dataset.on_the_fly.generation_batch_size=32 model_name=google/gemma-2-9b-it trainable_components.encoder.use_projection_layer=False checkpoint.strict_load=false +model_dtype=bfloat16 model_name=google/gemma-2-9b-it tokenizer_name=google/gemma-2-9b-it num_cpus_per_task=6 resume_reset_steps=false   unfreeze_encoder.enabled=true unfreeze_encoder.step=20000 
sleep 600
scripts/submit_with_config.sh config=conf/gemma2_WILDCHAT_9b_frozen_nopostfix.yaml run_suffix=NO_ENC_WILDCHAT_DIR_VARY_3 num_gpus_train=1 num_train_epochs=2 t_text=8 batch_size=16 group_n=8 trainable_components.encoder.output_layer=30 layer_l=30 gradient_accumulation_steps=1 gumbel_tau_schedule.constant_steps_before_decay=128os lm_base_weight=0 mse_weight=1.0 GRPO_weight=1.0 kl_base_weight=0 learning_rate=3e-4 weight_decay=0.00001 trainable_components.decoder.use_kv_cache=true lr_scheduler.warmup_steps=1000 freeze_schedule.enabled=false custom_lr_multipliers.overall_encoder=0.2 trainable_components.encoder.add_current_token=true trainable_components.decoder.projection_init_method=scale entropy_weight=0.5 verbose_samples.num_samples=2 verbose_samples.interval=2000 dataset.on_the_fly.enabled=true dataset.on_the_fly.samples_per_regeneration_cycle=1000 dataset.on_the_fly.max_val_samples=3000 nice=20000 trainable_components.decoder.patch_all_layers=3 resume_reset_steps=true wandb_resume_id=false grad_clip_enc=1 grad_clip=1 dataset.on_the_fly.generation_batch_size=32 model_name=google/gemma-2-9b-it trainable_components.encoder.use_projection_layer=False checkpoint.strict_load=false +model_dtype=bfloat16 model_name=google/gemma-2-9b-it tokenizer_name=google/gemma-2-9b-it num_cpus_per_task=6 resume_reset_steps=false  unfreeze_encoder.enabled=true unfreeze_encoder.step=20000 

sleep 600
scripts/submit_with_config.sh config=conf/gemma2_WILDCHAT_9b_frozen_nopostfix.yaml run_suffix=NO_ENC_WILDCHAT_DIR_VARY_5 num_gpus_train=1 num_train_epochs=2 t_text=8 batch_size=16 group_n=8 trainable_components.encoder.output_layer=30 layer_l=30 gradient_accumulation_steps=1 gumbel_tau_schedule.constant_steps_before_decay=128os lm_base_weight=0 mse_weight=1.0 GRPO_weight=1.0 kl_base_weight=0 learning_rate=3e-4 weight_decay=0.00001 trainable_components.decoder.use_kv_cache=true lr_scheduler.warmup_steps=1000 freeze_schedule.enabled=false custom_lr_multipliers.overall_encoder=0.2 trainable_components.encoder.add_current_token=true trainable_components.decoder.projection_init_method=scale entropy_weight=0.5 verbose_samples.num_samples=2 verbose_samples.interval=2000 dataset.on_the_fly.enabled=true dataset.on_the_fly.samples_per_regeneration_cycle=1000 dataset.on_the_fly.max_val_samples=3000 nice=20000 trainable_components.decoder.patch_all_layers=5 resume_reset_steps=true wandb_resume_id=false grad_clip_enc=1 grad_clip=1 dataset.on_the_fly.generation_batch_size=32 model_name=google/gemma-2-9b-it trainable_components.encoder.use_projection_layer=False checkpoint.strict_load=false +model_dtype=bfloat16 model_name=google/gemma-2-9b-it tokenizer_name=google/gemma-2-9b-it num_cpus_per_task=6 resume_reset_steps=false unfreeze_encoder.enabled=true unfreeze_encoder.step=20000 
 sleep 600
scripts/submit_with_config.sh config=conf/gemma2_WILDCHAT_9b_frozen_nopostfix.yaml run_suffix=NO_ENC_WILDCHAT_DIR_VARY_10 num_gpus_train=1 num_train_epochs=2 t_text=8 batch_size=16 group_n=8 trainable_components.encoder.output_layer=30 layer_l=30 gradient_accumulation_steps=1 gumbel_tau_schedule.constant_steps_before_decay=128os lm_base_weight=0 mse_weight=1.0 GRPO_weight=1.0 kl_base_weight=0 learning_rate=3e-4 weight_decay=0.00001 trainable_components.decoder.use_kv_cache=true lr_scheduler.warmup_steps=1000 freeze_schedule.enabled=false custom_lr_multipliers.overall_encoder=0.2 trainable_components.encoder.add_current_token=true trainable_components.decoder.projection_init_method=scale entropy_weight=0.5 verbose_samples.num_samples=2 verbose_samples.interval=2000 dataset.on_the_fly.enabled=true dataset.on_the_fly.samples_per_regeneration_cycle=1000 dataset.on_the_fly.max_val_samples=3000 nice=20000 trainable_components.decoder.patch_all_layers=10 resume_reset_steps=true wandb_resume_id=false grad_clip_enc=1 grad_clip=1 dataset.on_the_fly.generation_batch_size=32 model_name=google/gemma-2-9b-it trainable_components.encoder.use_projection_layer=False checkpoint.strict_load=false +model_dtype=bfloat16 model_name=google/gemma-2-9b-it tokenizer_name=google/gemma-2-9b-it num_cpus_per_task=6 resume_reset_steps=false  unfreeze_encoder.enabled=true unfreeze_encoder.step=20000 
 sleep 600
scripts/submit_with_config.sh config=conf/gemma2_WILDCHAT_9b_frozen_nopostfix.yaml run_suffix=NO_ENC_WILDCHAT_DIR_VARY_20 num_gpus_train=1 num_train_epochs=2 t_text=8 batch_size=16 group_n=8 trainable_components.encoder.output_layer=30 layer_l=30 gradient_accumulation_steps=1 gumbel_tau_schedule.constant_steps_before_decay=128os lm_base_weight=0 mse_weight=1.0 GRPO_weight=1.0 kl_base_weight=0 learning_rate=3e-4 weight_decay=0.00001 trainable_components.decoder.use_kv_cache=true lr_scheduler.warmup_steps=1000 freeze_schedule.enabled=false custom_lr_multipliers.overall_encoder=0.2 trainable_components.encoder.add_current_token=true trainable_components.decoder.projection_init_method=scale entropy_weight=0.5 verbose_samples.num_samples=2 verbose_samples.interval=2000 dataset.on_the_fly.enabled=true dataset.on_the_fly.samples_per_regeneration_cycle=1000 dataset.on_the_fly.max_val_samples=3000 nice=20000 trainable_components.decoder.patch_all_layers=20 resume_reset_steps=true wandb_resume_id=false grad_clip_enc=1 grad_clip=1 dataset.on_the_fly.generation_batch_size=32 model_name=google/gemma-2-9b-it trainable_components.encoder.use_projection_layer=False checkpoint.strict_load=false +model_dtype=bfloat16 model_name=google/gemma-2-9b-it tokenizer_name=google/gemma-2-9b-it num_cpus_per_task=6 resume_reset_steps=false unfreeze_encoder.enabled=true unfreeze_encoder.step=20000 
sleep 600
scripts/submit_with_config.sh config=conf/gemma2_WILDCHAT_9b_frozen_nopostfix.yaml run_suffix=NO_ENC_WILDCHAT_DIR_VARY_30 num_gpus_train=1 num_train_epochs=2 t_text=8 batch_size=16 group_n=8 trainable_components.encoder.output_layer=30 layer_l=30 gradient_accumulation_steps=1 gumbel_tau_schedule.constant_steps_before_decay=128os lm_base_weight=0 mse_weight=1.0 GRPO_weight=1.0 kl_base_weight=0 learning_rate=3e-4 weight_decay=0.00001 trainable_components.decoder.use_kv_cache=true lr_scheduler.warmup_steps=1000 freeze_schedule.enabled=false custom_lr_multipliers.overall_encoder=0.2 trainable_components.encoder.add_current_token=true trainable_components.decoder.projection_init_method=scale entropy_weight=0.5 verbose_samples.num_samples=2 verbose_samples.interval=2000 dataset.on_the_fly.enabled=true dataset.on_the_fly.samples_per_regeneration_cycle=1000 dataset.on_the_fly.max_val_samples=3000 nice=20000 trainable_components.decoder.patch_all_layers=30 resume_reset_steps=true wandb_resume_id=false grad_clip_enc=1 grad_clip=1 dataset.on_the_fly.generation_batch_size=32 model_name=google/gemma-2-9b-it trainable_components.encoder.use_projection_layer=False checkpoint.strict_load=false +model_dtype=bfloat16 model_name=google/gemma-2-9b-it tokenizer_name=google/gemma-2-9b-it num_cpus_per_task=6 resume_reset_steps=false  unfreeze_encoder.enabled=true unfreeze_encoder.step=20000 
 
