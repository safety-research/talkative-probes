# Configuration for Gemma3 27B K-sweep with 8 GPUs and caching
# Full node evaluation with vector caching for maximum efficiency

# Evaluation parameters
eval:
  # K values to evaluate
  k_values: [1, 2, 4, 8, 16, 32, 64, 128]  # More K values since it's fast with caching
  
  # Output settings
  output_dir: ./eval_results
  output_file: gemma3_k_sweep_8gpu_cache.json
  
  # Evaluation settings
  max_batches: null  # Use entire cached dataset
  temperature: 1.0
  
  # Batch size settings - optimized for 8 GPU distributed
  batch_size: 8
  max_generation_batch_size: 128  # Larger for distributed
  dataloader_batch_size: 256  # Larger for distributed
  vector_extraction_batch_size: 32  # Larger for distributed
  dataloader_fwd_pass_batch_size: 32  # Larger for distributed
  
  # Performance settings
  use_bf16: true
  
  # Sampling settings
  num_positions: 1
  max_val_samples: null  # Process entire dataset
  
  # Bootstrap disabled for speed
  do_bootstrap: false
  
  # Caching ENABLED for 8 GPU setup
  load_store: true
  cache_dir: vector_cache_8gpu

# Dataset configuration
dataset:
  activation_dir: "./data/SimpleStories_train"
  val_activation_dir: "./data/SimpleStories_test"
  
  on_the_fly:
    enabled: false
    generation_batch_size: ${eval.dataloader_fwd_pass_batch_size}

# Data loader settings
data:
  num_workers: 4  # More workers for 8 GPU setup

# Note: When using 8 GPUs with caching:
# - First run gathers all vectors to rank 0 and saves centralized cache
# - Subsequent runs can use ANY number of GPUs (1, 2, 4, 8, etc.)
# - Cached vectors are automatically distributed across available GPUs