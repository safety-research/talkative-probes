# Configuration for Gemma3 27B K-sweep with 8 GPUs and caching
# Full node evaluation with vector caching for maximum efficiency

# Evaluation parameters
eval:
  # K values to evaluate
  k_values: [1, 2, 4, 8, 16, 32, 64, 128]  # More K values since it's fast with caching
  
  # Output settings
  output_dir: ./eval_results
  output_file: gemma3_k_sweep_8gpu_cache.json
  
  # Evaluation settings
  max_batches: null  # Use entire cached dataset
  temperature: 1.0
  
  # Batch size settings - per GPU when using distributed (torchrun)
  batch_size: 8  # Legacy parameter, kept for compatibility
  max_generation_batch_size: 64  # For small lens model - can be large
  dataloader_batch_size: 128  # High-level dataloader batch size
  vector_extraction_batch_size: 16  # For large model forward pass if no pre-computed activations
  dataloader_fwd_pass_batch_size: 16  # For large model during dataset creation
  
  # Performance settings
  use_bf16: true
  
  # Sampling settings
  num_positions: 1
  max_val_samples: 100000  # Process up to 100k samples
  
  # Bootstrap disabled for speed
  do_bootstrap: false
  
  # Caching ENABLED for 8 GPU setup
  load_store: true
  cache_dir: vector_cache_8gpu

# Dataset configuration
dataset:
  activation_dir: "./data/SimpleStories_train"
  val_activation_dir: "./data/SimpleStories_test"
  
  on_the_fly:
    enabled: false
    generation_batch_size: 32

# Data loader settings
data:
  num_workers: 4  # More workers for 8 GPU setup

# Note: When using 8 GPUs with caching:
# - First run gathers all vectors to rank 0 and saves centralized cache
# - Subsequent runs can use ANY number of GPUs (1, 2, 4, 8, etc.)
# - Cached vectors are automatically distributed across available GPUs