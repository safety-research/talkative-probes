# Configuration for Gemma3 27B K-sweep with 8 GPUs (full node)
# Optimized for distributed evaluation across a full node

# Evaluation parameters
eval:
  # K values to evaluate
  k_values: [1, 2, 4, 8, 16, 32, 64]
  
  # Output settings
  output_dir: ./eval_results
  output_file: gemma3_k_sweep_8gpu.json
  
  # Evaluation settings
  max_batches: null  # Use entire validation set
  temperature: 1.0
  
  # Batch size settings - per GPU when using distributed (torchrun)
  batch_size: 8  # Legacy parameter, kept for compatibility
  max_generation_batch_size: 64  # For small lens model - can be large
  dataloader_batch_size: 128  # High-level dataloader batch size
  vector_extraction_batch_size: 16  # For large model forward pass if no pre-computed activations
  dataloader_fwd_pass_batch_size: 16  # For large model during dataset creation
  
  # Performance settings
  use_bf16: true
  
  # Sampling settings
  num_positions: 1
  max_val_samples: 100000  # Process up to 100k samples
  
  # Bootstrap disabled for speed
  do_bootstrap: false  # Bootstrap disabled by default for speed
  
  # Caching settings
  load_store: false
  cache_dir: vector_cache_8gpu

# Dataset configuration
dataset:
  activation_dir: "./data/SimpleStories_train"
  val_activation_dir: "./data/SimpleStories_test"
  
  on_the_fly:
    enabled: false
    generation_batch_size: 32

# Data loader settings
data:
  num_workers: 4  # More workers for 8 GPU setup