# Configuration for Gemma3 27B K-sweep with 8 GPUs (full node)
# Optimized for distributed evaluation across a full node

# Evaluation parameters
eval:
  # K values to evaluate
  k_values: [1, 2, 4, 8, 16, 32, 64]
  
  # Output settings
  output_dir: ./eval_results
  output_file: gemma3_k_sweep_8gpu.json
  
  # Evaluation settings
  max_batches: null  # Use entire validation set
  temperature: 1.0
  
  # Batch size settings - optimized for 8 GPU distributed
  batch_size: 8
  max_generation_batch_size: 128  # Larger for distributed
  dataloader_batch_size: 256  # Larger for distributed
  vector_extraction_batch_size: 32  # Larger for distributed
  dataloader_fwd_pass_batch_size: 32  # Larger for distributed
  
  # Performance settings
  use_bf16: true
  
  # Sampling settings
  num_positions: 1
  max_val_samples: null  # Process entire dataset with 8 GPUs
  
  # Bootstrap disabled for speed
  do_bootstrap: false
  
  # Caching settings
  load_store: false
  cache_dir: vector_cache_8gpu

# Dataset configuration
dataset:
  activation_dir: "./data/SimpleStories_train"
  val_activation_dir: "./data/SimpleStories_test"
  
  on_the_fly:
    enabled: false
    generation_batch_size: 32

# Data loader settings
data:
  num_workers: 4  # More workers for 8 GPU setup