# Configuration for GPT-2 Frozen Base Model Experiment
# This config trains consistency lens on GPT-2 with the base model frozen throughout

defaults:
  - gpt2_frozen
  - _self_

# Training hyperparameters
t_text: 2                       # Number of tokens to generate with the Decoder
decoder_prompt: "<|endoftext|>Short explanation of <embed>. Language, topic, sentiment, claims, speaker, style, etc:"
encoder_prompt: "<|endoftext|>Summary of the above text, encapsulating the main points and message, style and details:"
lm_loss_natural_prefix: "<|endoftext|>Short explanation of something. Language, topic, sentiment, claims, speaker, style, etc:"  # Natural language prefix for base model in LM loss

# Trainable components - FROZEN base model
trainable_components:
  decoder:
    base_model: true                 # FROZEN throughout
    projection_layer: true            # Train projection
    eye_init: true
    output_head: false                # Keep LM head frozen too
    use_flash_attention: false
    use_kv_cache: true
  encoder:
    base_model: true                 # FROZEN throughout
    use_base_model: true
    projection_layer: true            # Train projection
    eye_init: true
    output_layer: 6
    soft_prompt_init_text: ${encoder_prompt}

alpha_schedule: 
  type: "constant"
  value: 1.0

num_train_epochs: 20                   # 2 epochs should be plenty for OpenWebText

lm_base_weight: 0.0 
kl_base_weight: 1.0
mse_weight: 0.0

learning_rate: 1.0e-3
# Learning rate scheduler
lr_scheduler:
  type: "cosine"                      # Cosine annealing works well
  warmup_steps: 10                  # Number of *optimizer steps* for warmup.
                                    # Total micro-steps for warmup = warmup_steps * gradient_accumulation_steps.
                                    # E.g., 10 optimizer steps * 32 accum steps = 320 micro-steps for warmup.
  warmup_start_factor: 0.1            # Start at 10% of target LR
  eta_min: 1.0e-5   

batch_size: 512
gradient_accumulation_steps: 32
gumbel_tau_schedule:
  constant_steps_before_decay: 1000


freeze_schedule:
  enabled: false                       # ENABLE freeze schedule
  # unfreeze_at_step: 10000            # Unfreeze after 10k steps
  unfreeze_at_epoch: 2               # Unfreeze after first epoch
  # Alternative: unfreeze_at_epoch: 1 # Unfreeze after first epoch

# Custom learning rates - IMPORTANT for unfrozen models
custom_lr_multipliers:
  projection_layers: 1.0              # Full LR for projections
  prompt_layers: 1.0                 # 10x higher LR for prompt layers
  embedding_layers: 1
  base_models: 0.01
