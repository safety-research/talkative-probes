# Configuration for GPT-2 Frozen Base Model Experiment
# This config trains consistency lens on GPT-2 with the base model frozen throughout

defaults:
  - gpt2_unfreeze
  - _self_

# Training hyperparameters
t_text: 16                        # Number of tokens to generate with the Decoder

# Trainable components - FROZEN base model
trainable_components:
  encoder:
    output_layer: 6

alpha_schedule:
  type: "constant"
  value: 0.1

lm_base_weight: 1.0                  # Base LM weight; final LM multiplier = lm_base_weight * alpha_schedule  
kl_base_weight: 1.0             # Fixed weight on KL divergence (fundamental objective)
mse_weight: 0.0