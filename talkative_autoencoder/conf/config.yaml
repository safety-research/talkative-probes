# Configuration for Consistency Lens training

# Model parameters
# model_name: "sshleifer/tiny-gpt2" # Base model for LLM_orig, Decoder, Encoder
# Optional: specify a different tokenizer to use instead of the model's default
# E.g. a reduced-vocabulary tokenizer such as "Qilex/tinyStories-10k-tokenizer"
#tokenizer_name: "Qilex/tinyStories-10k-tokenizer"
model_name: "SimpleStories/SimpleStories-5M"
tokenizer_name: "SimpleStories/SimpleStories-5M"
# Optional: Use a different model for orig_model (e.g., chat-tuned variant)
# This allows using a different variant of the base model for computing interventions
# while keeping the encoder/decoder using the original model
# orig_model_name: "SimpleStories/SimpleStories-5M-Chat"  # Example: chat-tuned variant
orig_model_name: ${model_name}  # null = use same as model_name (default behavior)
orig_tokenizer_name: ${tokenizer_name}
# this is the layer idx!!!!
layer_l: 5                      # Target layer for activation extraction (LLM_orig)
#token_pos_p: 0                   # Target token position for activation extraction (LLM_orig) - currently fixed in MVP dumper


attn_implementation: null # apparently need this
# Training hyperparameters
t_text: 5                       # Number of tokens to generate with the Decoder
learning_rate: 1.0e-3
beta1: 0.9
beta2: 0.999
batch_size: 512               # Effective batch size (adjust with gradient_accumulation_steps if using DeepSpeed)
gradient_accumulation_steps: 1  # Number of steps to accumulate gradients before optimizer step

# Learning rate scheduler
lr_scheduler:
  type: "constant"              # Options: constant, linear, cosine, cosine_with_restarts, polynomial, exponential
  # Common parameters (not all apply to every scheduler type):
  warmup_steps: "100s"          # Warmup duration - Examples: "100s", "0.5e", "200steps", "1epoch"
  warmup_start_factor: 0.0000001      # Starting LR multiplier for warmup (lr * warmup_start_factor)
  # Scheduler-specific parameters:
  # For linear:
  # end_factor: 0.0             # Final LR multiplier (lr * end_factor)
  # For cosine:
  # eta_min: 0.0                # Minimum learning rate
  # For cosine_with_restarts:
  # eta_min: 0.0                # Minimum learning rate
  # T_0: "500s"                 # Duration until first restart - Examples: "500s", "1e", "1000steps", "2epochs"
  # T_mult: 2                   # Factor to increase T_0 after a restart
  # For polynomial:
  # power: 1.0                  # Polynomial power (1.0 = linear decay)
  # end_factor: 0.0             # Final LR multiplier
  # For exponential:
  # gamma: 0.95                 # Multiplicative factor of learning rate decay

# Training duration - use either epochs OR steps, not both:
# For epoch-based training:
num_train_epochs: 10            # Number of epochs to train (set > 0 for epoch-based)
max_train_steps: 0              # Set to 0 when using num_train_epochs
# For step-based training:
# num_train_epochs: 0           # Set to 0 when using max_train_steps  
# max_train_steps: 50000        # Number of training steps (set > 0 for step-based)

# Early termination (useful for sweeps and testing)
early_termination:
  enabled: false                # Enable early termination
  steps: null                   # Terminate after N steps - Examples: "1000s", "100os", "5000steps"
                               # This overrides max_train_steps/num_train_epochs if set
compile_models: true             # Wrap Decoder & Encoder with torch.compile if True
log_interval: "10s"           # Log metrics interval - Examples: "10s", "0.1e", "100steps", "1epoch"
wandb_log_interval: "1s"      # WandB log interval - Examples: "1s", "0.01e", "10steps"
grad_clip: 1.0
grad_clip_enc: null

# Update norm calculation (for distributed training)
compute_update_norm: true     # Whether to compute parameter update norms at optimizer steps

stop_grad_aprime: false

# Decoder parameters
decoder_prompt: "<|endoftext|>short explanation of <embed>:"       # Prompt for the Decoder
lm_loss_natural_prefix: "<|endoftext|>short explanation of something:"  # Natural language prefix for base model in LM loss
encoder_prompt: "<|endoftext|>Summary of the text:"
# Trainable components configuration
trainable_components:
  decoder:
    base_model: false          # Train the underlying LLM weights of the Decoder
    projection_layer: true   # Train Proj_A_to_D_emb (self.proj)
    output_head: false # Train the Decoder's output LM head (self.out)
    embedding_head: false      # Train the input/output embeddings of the base model (may be tied)
    pos_embeddings: false
    eye_init: true
    trainable_prompts: true   # Train the decoder's soft prompt embeddings
    use_checkpointing: false  # Use generate_soft_chkpt with gradient checkpointing
    checkpoint_every_n_tokens: 1          # Checkpoint frequency: 1 = every token (max savings), higher = less frequent
    use_kv_cache: false       # Use generate_soft_kv_cached for O(n) attention computation (GPT-2 only)
    use_gumbel_for_LMorig: false  # Use Gumbel-Softmax outputs from decoder to feed the original LM for the LM loss
    patch_all_layers: false
    per_layer_projections: false
    use_dropout: false        # Whether to use dropout during training (False = deterministic, recommended for frozen models)
    detach_after_each_sample: false
    end_to_end: true
    projection_init_method: "scale"  # Options: "default", "tuned_lens_composition"
    # TunedLens initialization (when projection_init_method = "tuned_lens_composition"):
    # RECOMMENDED: Leave checkpoint null to auto-load pre-trained lens from HuggingFace
    projection_init_tl_checkpoint: null  # null = auto-load, or specify "username/model-tuned-lens" or "/path/to/checkpoint"
    projection_init_tl_model_name: null  # null = use main model_name, or specify different model
    projection_init_tl_comp_src_layer: null  # Not used - src layer is always layer_l from config
    projection_init_tl_comp_tgt_layer: 0  # Target decoder layer (for single projection mode)
    projection_init_tl_comp_use_composed_bias: true
    subtract_add_pos_embeddings: false
    extra_pos_embeddings: false
    clamp_entropy: -9999 # or 0.5 etc
    attn_implementation: ${attn_implementation} # apparently need this
  encoder:
    base_model: false          # Train the underlying LLM weights of the Encoder
    use_base_model: true      # Use the base model to process the embeddings
    use_projection_layer: true   # Train Proj_E_hidden_to_A (self.proj)
    projection_layer: true   # Train Proj_E_hidden_to_A (self.proj)
    embedding_head: false      # Train the input/output embeddings of the base model (may be tied)
    eye_init: true
    soft_prompt_length: 0     # Number of trainable soft prompt tokens (0 to disable)
    trainable_soft_prompt: true  # Whether soft prompt tokens are trainable
    soft_prompt_init_std: 0.1  # Standard deviation for random initialization of soft prompt tokens
    soft_prompt_init_text: ${encoder_prompt} # Text to initialize soft prompt from (overrides random init, e.g. "reconstruct activation:")
    output_layer: -1          # Which layer to extract encoder activations from (-1 = last layer, 0 = embeddings, 1..n = specific layer)
    use_dropout: false        # Whether to use dropout during training (False = deterministic, recommended for frozen models)
    projection_init_method: "scale"
    subtract_add_pos_embeddings: false  
    extra_pos_embeddings: false
    add_current_token: false
    special_last_token_vector: false
    attn_implementation: ${attn_implementation} # apparently need this
# Freeze schedule for non-adapter parameters
# When enabled, non-adapter parameters (base_model, output_head, embedding_head) will be frozen
# for the specified time, then unfrozen for the rest of training
# 
# FLEXIBLE SCHEDULING: Use suffix notation for epochs/steps:
#   - "1000s" or "1000steps" = 1000 steps
#   - "5e" or "5epochs" = 5 epochs  
#   - Plain numbers (legacy) = steps
freeze_schedule:
  enabled: false                # Enable freeze schedule
  
  # Global unfreeze timing (applies to all components unless overridden)
  unfreeze_at: "1000s"         # When to unfreeze non-adapter parameters
                               # Examples: "1000s", "5e", "2000steps", "10epochs"
  
  # Warmup settings for newly unfrozen parameters
  warmup_duration: "100s"      # Duration to warmup newly unfrozen parameters
  warmup_start_factor: 0.01    # Starting LR multiplier for warmup (1% of target LR)
  # The learning rate will linearly increase from start_factor * lr to full lr over warmup_duration
  
  # Fine-grained component control with individual timing
  # Each component can have its own unfreeze timing that overrides the global unfreeze_at
  components:
    decoder:
      base_model:
        enabled: null          # null = use trainable_components setting, true/false = override
        unfreeze_at: null      # null = use global unfreeze_at, or specify like "500s", "3e"
      output_head:
        enabled: null
        unfreeze_at: null      # Can unfreeze at different time than base_model
      embedding_head:
        enabled: null
        unfreeze_at: null      # Embedding heads can have their own schedule
      soft_prompt_embeddings:
        enabled: null
        unfreeze_at: null
    encoder:
      base_model:
        enabled: null
        unfreeze_at: null
      embedding_head:
        enabled: null
        unfreeze_at: null
      soft_prompt_embeddings:
        enabled: null
        unfreeze_at: null
        
  # Legacy compatibility (deprecated - use unfreeze_at instead)
  # unfreeze_at_step: 1000     # Step at which to unfreeze (converted to "1000s")
  # unfreeze_at_epoch: 2       # Epoch at which to unfreeze (converted to "2e")

# Learning rate multiplier for specific parameter groups.
# These multipliers are applied to the base learning_rate from above.
# The training script will need to create parameter groups accordingly.
# Example: projection_layers refers to decoder.proj, decoder.out, and encoder.proj.
custom_lr_multipliers:
  projection_layers: 1.0 # Set > 1.0 to use a higher LR for these layers.
  embedding_layers: 1.0  # Set different LR for embedding heads (input/output embeddings)
  prompt_layers: 1.0     # Set different LR for prompt layers
  base_models: 1.0        # Set different LR for base model parameters
  overall_encoder: 1.0     # Set different LR for encoder layers

# Gumbel-Softmax temperature (tau) schedule
gumbel_tau_schedule:
  type: "linear_decay_after_constant"  # Options: constant, linear_decay, cosine_anneal, exponential_decay, linear_decay_after_constant
  constant_steps_before_decay: "5000s"  # Duration of constant phase - Examples: "5000s", "2e", "10000steps", "5epochs"
  start_value: 1.0
  end_value: 0.1
  num_steps: "-1"       # Duration of schedule - "-1" = full training, or use "10000s", "5e", etc.

GRPO_beta_schedule:
  type: "constant"
  value: ${GRPO_beta}
  constant_steps_before_decay: "400os"
  start_value: ${GRPO_beta}
  end_value: 0
  num_steps: "1000os"

# Language model loss weight (alpha) schedule  
# Alpha ramps up the LM loss component to gradually introduce linguistic fluency constraints
alpha_schedule:
  type: "constant"      # Options: constant, linear_warmup, etc.
  value: 0.1            # For constant schedule
  # For linear_warmup:
  # start_value: 0.0
  # end_value: 0.1
  # num_steps: "50s"     # Duration - Examples: "50s", "0.5e", "100steps", "1epoch"

# Loss component weights (these are currently hardcoded in loop.py, could move here)
# loss_ce_weight: 0.01
# loss_kl_weight: 0.1 # This is superseded by alpha_schedule if used as the direct multiplier

# Activation dumper config (for 00_dump_activations.py)
pretokenize:
  output_dir: null 
  num_proc: 32
  batch_size: 10000
  force: true
  run_before_training: false
  name_of_sub_dataset: null

activation_dumper:
  num_samples: -1  # -1 means process entire dataset
  min_pos: 5
  use_hf_dataset: true
  hf_dataset_name: "SimpleStories/SimpleStories"
  hf_split: "train"
  dataset_cache_dir: "./data/corpus/SimpleStories"
  output_dir: "./data/SimpleStories_train"
  batch_size: 2048  # With pre-tokenization, we can use larger batches
  # Validation split dumping in the same run of 00_dump_activations.py
  val_hf_split: "test"
  val_output_dir: "./data/SimpleStories_test"
  val_num_samples: -1  # -1 means process entire validation dataset
  use_pretokenized: false
  pretokenized_path: null
  seq_len: 64 # This will be used by on-the-fly logic if not overridden by a more specific seq_len for OTF.
  samples_per_shard: 10000

  # HuggingFace dataset loading options
  # If 'use_hf_dataset' is true the dumper script will read the tiny 10-k subset of
  # The Pile instead of the built-in toy prompts. Useful once the smoke test passes
  # and we want more realistic activations.
  # NOTE: the dumper currently loads the entire split into RAM; see TODO in the
  #       script about switching to streaming=True for larger datasets.
  # future option – not yet consumed by the script
  # streaming: false

# Validation activation directory (skip split when provided)
# If specified, this overrides automatic train/val splitting
# NOTE: This should match activation_dumper.val_output_dir above
val_activation_dir: null  # Set to path to use pre-split validation data


# WandB configuration (can also be in wandb.yaml)
wandb:
  project: "consistency-lens-mvp"
  mode: "online" # "online", "offline", "disabled"

# Checkpoint configuration
checkpoint:
  enabled: true
  base_output_dir: "outputs/checkpoints"  # Base directory for checkpoints
  output_dir: null # will be set to base_output_dir / run_name
  
  # Checkpoint naming pattern - supports {step}, {epoch}, {val_loss}, {timestamp}
  name_pattern: "checkpoint_step{step}_epoch{epoch}"
  
  # Save strategies (can enable multiple)
  save_every_n_steps: "1000s"  # Save interval - Examples: "20000s", "5e", "10000steps", "2epochs" (use "0" to disable)
  save_every_n_epochs: 0        # Legacy: Save every N epochs (0 to disable) - prefer save_every_n_steps with "Ne"
  save_at_end: true             # Save final checkpoint when training completes
  
  # Best checkpoint tracking
  track_best_n: 3                 # Keep N best checkpoints based on validation loss (0 to disable)
  best_metric: "val_loss"         # Metric to track for best checkpoints
  best_mode: "min"                # "min" or "max" depending on metric
  
  # What to save in checkpoints
  save_components:
    models: true                  # Save model state dicts
    optimizer: true               # Save optimizer state
    scheduler: true               # Save scheduler state (if using)
    config: true                  # Save config used for this run
    metrics: true                 # Save current metrics
    
  # Checkpoint management
  max_checkpoints: 3             # Maximum total checkpoints to keep (-1 for unlimited)
  delete_old_checkpoints: true    # Delete oldest checkpoints when max_checkpoints reached
  strict_load: true

# Loss weights
lm_base_weight: 1.0                  # Base LM weight; final LM multiplier = lm_base_weight * alpha_schedule  
kl_base_weight: 1.0             # Fixed weight on KL divergence (fundamental objective)
mse_weight: 0.0                 # Weight for MSE reconstruction loss (0.0 to disable, use instead of KL for direct reconstruction)
GRPO_weight: 0.0
GRPO_beta: 0.0
GRPO_entropy_weight: 0.0
skip_tokens_KL_GRPO: 0

# Token entropy bonus (see loop.py). Positive values reward higher entropy to
# avoid posterior collapse; set to 0.0 to disable.
entropy_weight: 0.000

# Entropy weight schedule - dynamically adjust entropy bonus during training
# This allows for better control over token diversity throughout training
entropy_schedule:
  type: "constant"      # Options: constant, linear_warmup, linear_decay, linear_decay_after_constant, cosine_anneal, exponential_decay
  value: 0.0            # For constant schedule (same as entropy_weight above for backward compatibility)
  # For linear_decay_after_constant (recommended - start high to prevent collapse, then decay):
  # type: "linear_decay_after_constant"
  # constant_steps_before_decay: "1000s"  # Keep high entropy for initial steps
  # start_value: 0.01     # Initial high entropy bonus
  # end_value: 0.001      # Final lower entropy bonus
  # num_steps: "5000s"    # Total duration (includes constant phase)
  # For linear_warmup:
  # start_value: 0.0
  # end_value: 0.01
  # num_steps: "2000s"   # Duration - Examples: "2000s", "1e", "5000steps", "2epochs"
  # For linear_decay:
  # start_value: 0.01
  # end_value: 0.0
  # num_steps: "-1"      # "-1" = full training duration

clamp_entropy: null # deprecated - see schedule
entropy_clamp_schedule:
  type: "linear_decay_after_constant"
  constant_steps_before_decay: "1000s"
  start_value: 2.0
  end_value: 0.1
  num_steps: "5000os"
  value: 0.1

# Validation split & scheduling
val_fraction: 0.1               # Fraction of dataset for validation
split_seed: 42                  # Random seed for train/val split
val_interval: "250s"            # Run validation interval using flexible notation
                                # Examples: "100s", "1e", "500steps", "2epochs"

# Verbose sample configuration for training monitoring
verbose_samples:
  enabled: true                 # Enable verbose sample printing during training
  num_samples: 5               # Number of samples to print per interval
  interval: "1000s"            # Print interval using flexible notation
                               # Examples: "1000s", "1e", "500steps", "2epochs"
  top_n_predictions: 3          # Number of top predictions to show in analysis
  generate_continuation: true   # Generate autoregressive continuation
  continuation_tokens: 30       # Number of tokens to generate for continuation
  compare_with_tuned_lens: true # Set to true to enable this feature
  tuned_lens_checkpoint_path_or_name: null # Or "AlignmentResearch/tuned-lens-pythia-160m", or "/local/path/to/tl_checkpoint"
                                         # If null, will try to auto-load from HuggingFace based on model_name
  # WandB table management
  wandb_table_limit: true       # Keep only first 1 and last 3 rows in wandb table to save memory
  
  # Legacy compatibility (deprecated - use interval instead)
  # interval_type: "steps"      # "steps" or "epochs" - when to print samples
  # interval: 1000              # Print every N steps/epochs (based on interval_type)

# ------------------------------------------------------------------------------
# Optional CLI-style parameters (set to null by default)
# ------------------------------------------------------------------------------
resume: null            # Path to checkpoint to resume from
wandb_resume_id: null   # WandB run ID when resuming
run_name: null          # Manually set run name (otherwise auto-generated)
activation_dir: null    # Override activation_dumper.output_dir
max_train_samples: null  # Cap training dataset size
max_val_samples: null    # Cap validation dataset size
resume_reset_steps: false
strict_optimizer_load: true

# Smooth LR transition when resuming from checkpoint
# When enabled and the checkpoint LR differs from config LR, this will
# smoothly transition between them instead of an abrupt change
smooth_lr_transition:
  enabled: false        # Enable smooth LR transition on resume
  transition_steps: "1000s"  # Duration of transition - Examples: "1000s", "1e", "500steps", "2epochs"

# ------------------------------------------------------------------------------
# Evaluation configuration (for 02_eval.py)
# ------------------------------------------------------------------------------
evaluation:
  activation_dir: null  # Override activation directory for evaluation (optional)
  batch_size: 4         # Batch size for evaluation
  num_batches: 25       # Number of batches to evaluate
  verbose_samples: 3    # Number of verbose samples to print
  top_n_analysis: 3     # Number of top-N predictions to show in verbose analysis
  val_fraction: null    # Fraction of dataset for validation (null = use main config)
  split_seed: null      # Seed for train/val split (null = use main config)
  output_dir: null      # Directory to save evaluation results (null = auto-generate)
  save_results: false   # Save evaluation results to JSON file
  
  # Custom string evaluation options (for analyzing arbitrary text)
  custom_strings: []       # List of strings to analyze, e.g., ["Hello world", "Test string"]
  custom_strings_file: null  # Path to file containing strings (one per line)
  analyze_positions: "last"  # Which positions to analyze: "last", "all", "first", or list of indices

# ------------------------------------------------------------------
# Parameter-drift logging
# ------------------------------------------------------------------
parameter_drift:
  enabled: true        # Set false to disable all drift logging
  interval: "1000s"   # Flexible schedule – examples: "10000s", "1e", "500steps"

run_suffix: ""
resample_ablation: true

# Mixed precision training configuration
mixed_precision:
  enabled: true  # Enable automatic mixed precision
  dtype: "auto"  # Options: "auto", "float16", "bfloat16", "float32"
               # "auto" will use bfloat16 if available, otherwise float32 (safest option)

weight_decay: 0.01
force_data_conversion: false

group_n: 1
do_kl_computation: true
do_lm_computation: true

allow_reset_optimizer_on_resume: false
detect_anomaly: false

# Inside your main config.yaml or an experiment file

dataset:
  # ... existing dataset configs like train_split, val_split ...
  on_the_fly:
    enabled: false # SET TO TRUE TO ENABLE ON-THE-FLY GENERATION
    generation_batch_size: 32 # Batch size for activation generation
    position_selection_strategy: "random" # "random" or "midpoint"
    pretokenized_path: "data/pretokenized/your_dataset_name_seqXXX" # REQUIRED if enabled: Path to the root of HF dataset saved by `load_from_disk`
    layer_l: {layer_l} # Layer from which to extract activations (e.g., 0 for first layer after embeddings)
    min_pos: 8 # Minimum token position (0-indexed) to consider for activation extraction
    # seq_len will be taken from activation_dumper.seq_len or a global seq_len if defined
    max_val_samples: 50000
    validation_samples_override: null # Or an integer, e.g., 1000. If null, uses max_val_samples or activation_dumper.val_num_samples.
                                     # Set to 0 to disable on-the-fly validation data generation specifically.
                                     

    training_cache:
      type: "memory" # "memory" or "disk"
      size_samples: 10000 # Number of training (A) activations to buffer in the cache per fill cycle
      disk_path: "/path/to/disk_cache_location" # REQUIRED if type is "disk". Unique per run is good.
                                                # The script will append /rank_X/ to this.
  preload_to_shared_ram: false
  use_mmap: false # ensure preload_to_shared_ram is false if this is true

# # Ensure activation_dumper config still has seq_len if not defined globally,
# # as on_the_fly_config might implicitly use it.
# activation_dumper:
#   # ... other activation_dumper settings ...
#   seq_len: 64 # This will be used by on-the-fly logic if not overridden by a more specific seq_len for OTF.

# It's also good practice to ensure these are defined if you rely on them as fallbacks
# for num_val_samples_to_generate in _prepare_dataloaders
#max_val_samples: 50000 # Or null
# activation_dumper.val_num_samples: 5000 # (already part of activation_dumper)
do_soft_token_embeds: true
mean_n_sequences: false