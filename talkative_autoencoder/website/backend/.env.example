# Model Configuration
CHECKPOINT_PATH=/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_WILDCHAT_9b_frozen_nopostfix_GG29AW1S_gemma-2-9b-it_L30_e30_frozen_lr1e-4_t8_2ep_resume_0723_124516_NO_ENC_PROJ8_CHAT_DIR_OTF_dist2_slurm6696
DEVICE=cuda
BATCH_SIZE=32
USE_BF16=true

# API Configuration
ALLOWED_ORIGINS=http://localhost:3000,https://your-frontend.vercel.app
API_KEY=  # Optional API key for authentication
MAX_QUEUE_SIZE=100
MAX_TEXT_LENGTH=1000
AUTO_BATCH_SIZE_MAX=512  # Maximum value for auto-calculated batch size (256Ã·N)
LAZY_LOAD_MODEL=false  # Load model on startup (false) or on first request (true)

# Redis Configuration (Optional)
# REDIS_URL=redis://localhost:6379
# CACHE_TTL=3600

# RunPod Configuration (Optional)
# RUNPOD_POD_ID=your-pod-id
# RUNPOD_API_KEY=your-api-key
