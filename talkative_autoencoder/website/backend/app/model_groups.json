{
  "model_groups": [
    {
      "group_id": "gemma2-9b-it",
      "group_name": "Gemma 2 9B IT",
      "base_model_path": "google/gemma-2-9b-it",
      "description": "Gemma 2 9B instruction-tuned variants (42 layers)",
      "visible": true,
      "models": [
        {
          "id": "gemma2-9b-wildchat",
          "name": "Gemma-2-9B-IT WildChat L30",
          "OLDlens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_WILDCHAT_9b_frozen_nopostfix_GG29AW1S_gemma-2-9b-it_L30_e30_frozen_lr1e-4_t8_2ep_resume_0723_124516_NO_ENC_PROJ8_CHAT_DIR_OTF_dist2_slurm6696",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_WILDCHAT_9b_frozen_nopostfix_GG29AW1S_gemma-2-9b-it_L30_e30_frozen_lr1e-4_t16_2ep_resume_0806_160535_NO_ENC_PROJ8_CHAT_DIR_LONGER_WIDER_OTF_dist8_slurm7180",
          "different_activations_orig_path": null,
          "layer": 30,
          "description": "Trained on WildChat dataset (ONLY)",
          "batch_size": 48,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma2-9b-chat-l10",
          "name": "Gemma-2-9B-IT Chat L10",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_CHAT_9b_frozen_nopostfix_GG29SUS_gemma-2-9b-it_L10_e10_frozen_lr1e-4_t8_2ep_resume_0723_084137_ITBASE_NO_ENC_PROJ_NOW10_OTF_dist2_slurm6692",
          "different_activations_orig_path": null,
          "layer": 10,
          "description": "Openwebtext + Chat tuned, layer 10 finetuned from layer 30",
          "batch_size": 64,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma2-9b-chat-l20",
          "name": "Gemma-2-9B-IT Chat L20",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_CHAT_9b_frozen_nopostfix_GG29SUS_gemma-2-9b-it_L20_e20_frozen_lr1e-4_t8_2ep_resume_0723_084137_ITBASE_NO_ENC_PROJ_NOW20_OTF_dist2_slurm6693",
          "different_activations_orig_path": null,
          "layer": 20,
          "description": "Openwebtext + Chat tuned, layer 20 finetuned from layer 30",
          "batch_size": 48,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma2-9b-chat-l30",
          "name": "Gemma-2-9B-IT Chat L30",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_CHAT_9b_frozen_nopostfix_GG29SUS_gemma-2-9b-it_L30_e30_frozen_lr3e-4_t8_2ep_resume_0718_132614_ITBASE_NO_ENC_PROJ_OTF_dist4_slurm6570",
          "different_activations_orig_path": null,
          "layer": 30,
          "description": "Openwebtext + Chat tuned, layer 30",
          "batch_size": 48,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma2-9b-chat-l36",
          "name": "Gemma-2-9B-IT Chat L36",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_CHAT_9b_frozen_nopostfix_GG29SUS_gemma-2-9b-it_L30_e36_frozen_lr1e-4_t8_2ep_resume_0718_131554_ITBASE_NO_ENC_PROJ_NOW36_OTF_dist2_slurm6561",
          "different_activations_orig_path": null,
          "layer": 36,
          "description": "Openwebtext + Chat tuned, layer 36 finetuned from layer 30",
          "batch_size": 48,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma2-9b-chat-shallow5",
          "name": "Gemma-2-9B-IT Chat Shallow Out layer 30 -> layer 5",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_CHAT_9b_frozen_nopostfix_GG29SUS_gemma-2-9b-it_L30_e5_frozen_lr1e-4_t8_2ep_resume_0718_132510_ITBASE_NO_ENC_PROJ_SHALLOWOUT_OTF_dist2_slurm6571",
          "different_activations_orig_path": null,
          "layer": 30,
          "description": "Openwebtext + Chat tuned, layer 30, output layer of the encoder is layer 5 (so very limited computation)",
          "batch_size": 64,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        }
      ]
    },
    {
      "group_id": "gemma3-27b-it",
      "group_name": "Gemma 3 27B IT",
      "base_model_path": "google/gemma-3-27b-it",
      "description": "Gemma 3 27B instruction-tuned models (62 layers)",
      "visible": true,
      "models": [
        {
          "id": "gemma3-27b-chat",
          "name": "Gemma-3-27B-IT Chat L45",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma3_CHAT_27b_frozen_nopostfix_GG32PAW1S_gemma-3-27b-it_L45_e45_frozen_lr3e-4_t8_4ep_resume_0724_191127_frozenenc_add_patch5_suffix1p0enc_NO_PROJ_IT_E_D_OTF_dist8",
          "different_activations_orig_path": null,
          "layer": 45,
          "description": "Trained on FineWeb + finetuned on WildChat data",
          "batch_size": 16,
          "auto_batch_size_max": 512,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma3-27b-wildchat-only",
          "name": "Gemma-3-27B-IT WildChat L45 (only)",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma3_CHAT_27b_frozen_nopostfix_GG32PAW1S_gemma-3-27b-it_L45_e45_frozen_lr1e-3_t8_4ep_resume_0804_091446_frozenenc_add_patch5_suffix1p0enc_NO_PROJ_WILD_CHATDIR_OTF_dist8",
          "different_activations_orig_path": null,
          "layer": 45,
          "description": "Trained only on WildChat data",
          "batch_size": 16,
          "auto_batch_size_max": 512,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma3-27b-chat-l30",
          "name": "Gemma-3-27B-IT Chat L30",
          "OLDlens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma3_CHAT_27b_frozen_nopostfix_GG32PAW1S_gemma-3-27b-it_L30_e30_frozen_lr3e-4_t8_4ep_resume_0730_195708_frozenenc_add_patch5_suffix1p0enc_NO_PROJ_IT_E_D_L30tune_OTF_dist8_slurm6704",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma3_CHAT_27b_frozen_nopostfix_GG32PAW1S_gemma-3-27b-it_L30_e30_frozen_lr1e-3_t8_4ep_resume_0805_073520_frozenenc_add_patch5_suffix1p0enc_NO_PROJ_WILD_CHATDIR_30_OTF_dist8",
          "different_activations_orig_path": null,
          "layer": 30,
          "description": "Trained on FineWeb + finetuned on WildChat data (L30 finetuned from L45)",
          "batch_size": 16,
          "auto_batch_size_max": 512,
          "visible": true,
          "backend_only": false
        }
      ]
    },

    {
      "group_id": "qwen2.5-14b",
      "group_name": "Qwen 2.5 14B",
      "base_model_path": "Qwen/Qwen2.5-14B-Instruct",
      "description": "Qwen 2.5 14B instruction models",
      "visible": true,
      "models": [
        {
          "id": "qwen2.5-14b-wildchat",
          "name": "Qwen 2.5 14B WildChat L36",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/qwen2_5_WCHAT_14b_frozen_nopostfix_QQ1IAW1S_Qwen2.5-14B-Instruct_L36_e36_frozen_lr1e-3_t8_4ep_resume_0729_073454_NO_ENC_PROJ8_higherELR_PT_TO_CHAT_OTF_dist4_slurm6708",
          "different_activations_orig_path": null,
          "layer": 36,
          "description": "FineWeb pretrained, WildChat finetuned",
          "batch_size": 32,
          "auto_batch_size_max": 2048,
          "visible": true,
          "backend_only": false
        }
      ]
    },
    {
      "group_id": "qwen2.5-14b-EM",
      "group_name": "Qwen 2.5 14B EM",
      "base_model_path": "Qwen/Qwen2.5-14B-Instruct",
      "description": "Qwen 2.5 14B instruction models with EM donor (no finetuning on misaligned model vectors)",
      "visible": true,
      "models": [
        {
          "id": "qwen2.5-14b-wildchat-EM",
          "name": "Emergently misaligned Qwen 2.5 14B on WildChat L36",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/qwen2_5_WCHAT_14b_frozen_nopostfix_QQ1IAW1S_Qwen2.5-14B-Instruct_L36_e36_frozen_lr1e-3_t8_4ep_resume_0729_073454_NO_ENC_PROJ8_higherELR_PT_TO_CHAT_OTF_dist4_slurm6708",
          "IGNOREdifferent_activations_orig_path": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_bad-medical-advice",
          "different_activations_orig_path": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_risky-financial-advice",
          "IGNORE2different_activations_orig_path": "ModelOrganismsForEM/Qwen2.5-14B-Instruct_full-ft",
          "layer": 36,
          "description": "FineWeb pretrained, WildChat finetuned, applied to EM model trained on financial advice",
          "batch_size": 32,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        }
      ]
    },
    {
      "group_id": "gemma2-9b-it-abliterated",
      "group_name": "Gemma 2 9B IT abliterated",
      "base_model_path": "google/gemma-2-9b-it",
      "description": "Gemma 2 9B instruction-tuned variants abliterated (42 layers)",
      "visible": true,
      "models": [
        {
          "id": "gemma2-9b-wildchat-abliterated",
          "name": "Gemma-2-9B-IT WildChat L30 abliterated",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_WILDCHAT_9b_frozen_nopostfix_GG29AW1S_gemma-2-9b-it_L30_e30_frozen_lr1e-4_t8_2ep_resume_0801_120554_NO_ENC_PROJ8_CHAT_DIR_LONGER_OTF_dist8_slurm6901",
          "different_activations_orig_path": "IlyaGusev/gemma-2-9b-it-abliterated",
          "layer": 30,
          "description": "Trained on WildChat dataset (ONLY), layer 30 (42 total)",
          "batch_size": 48,
          "auto_batch_size_max": 1024,
          "visible": true,
          "backend_only": false
        }
      ]
    },
    {
      "group_id": "gemma2-9b-it-auditing-target",
      "group_name": "Gemma 2 9B IT audit target (NOT FOR THE AUDITING GAME)",
      "base_model_path": "google/gemma-2-9b-it",
      "description": "Gemma 2 9B target (42 layers)",
      "visible": true,
      "models": [
        {
          "id": "gemma2-9b-target-l30-1",
          "name": "Gemma-2-9B-IT L30",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_WILDCHAT_9b_frozen_nopostfix_GG29AW1S_gemma-2-9b-it_L30_e30_frozen_lr1e-4_t8_2ep_resume_0801_120554_NO_ENC_PROJ8_CHAT_DIR_LONGER_OTF_dist8_slurm6901",
          "different_activations_orig_path": "IlyaGusev/gemma-2-9b-it-abliterated",
          "layer": 30,
          "description": "Trained autoencoder on WildChat dataset (ONLY), layer 30 (42 total)",
          "batch_size": 48,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        }
      ]
    },
    {
      "group_id": "gemma2-9b-it-auditing-target-2",
      "group_name": "Gemma 2 9B IT audit target 2 (NOT FOR THE AUDITING GAME)",
      "base_model_path": "google/gemma-2-9b-it",
      "description": "Gemma 2 9B target (42 layers)",
      "visible": true,
      "models": [
        {
          "id": "gemma2-9b-target-l30-2",
          "name": "Gemma-2-9B-IT L30",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_WILDCHAT_9b_frozen_nopostfix_GG29AW1S_gemma-2-9b-it_L30_e30_frozen_lr1e-4_t8_2ep_resume_0801_120554_NO_ENC_PROJ8_CHAT_DIR_LONGER_OTF_dist8_slurm6901",
          "different_activations_orig_path": "bcywinski/gemma-2-9b-it-taboo-smile",
          "layer": 30,
          "description": "Trained autoencoder on WildChat dataset (ONLY), layer 30 (42 total)",
          "batch_size": 48,
          "auto_batch_size_max": 768,
          "visible": false,
          "backend_only": true
        }
      ]
    },
    {
      "group_id": "gemma2-9b-it-auditing-secret",
      "group_name": "Gemma 2 9B IT auditing game",
      "base_model_path": "google/gemma-2-9b-it",
      "description": "Gemma 2 9B secret (42 layers)",
      "visible": true,
      "models": [
        {
          "id": "gemma2-9b-secret-l30-secret",
          "name": "Gemma-2-9B-IT L30 - AUDITING GAME - RECOMMENDED",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_WILDCHAT_9b_frozen_nopostfix_GG29AW1S_gemma-2-9b-it_L30_e30_frozen_lr1e-4_t8_2ep_resume_0801_120554_NO_ENC_PROJ8_CHAT_DIR_LONGER_OTF_dist8_slurm6901",
          "different_activations_orig_path": "bcywinski/gemma-2-9b-it-secret1",
          "layer": 30,
          "description": "Trained autoencoder on WildChat dataset (ONLY), layer 30 (42 total)",
          "batch_size": 48,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma2-9b-wildchat-secret",
          "name": "Gemma-2-9B-IT WildChat L30 - AUDITING GAME - 16 tokens",
          "OLDlens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_WILDCHAT_9b_frozen_nopostfix_GG29AW1S_gemma-2-9b-it_L30_e30_frozen_lr1e-4_t8_2ep_resume_0723_124516_NO_ENC_PROJ8_CHAT_DIR_OTF_dist2_slurm6696",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_WILDCHAT_9b_frozen_nopostfix_GG29AW1S_gemma-2-9b-it_L30_e30_frozen_lr1e-4_t16_2ep_resume_0806_160535_NO_ENC_PROJ8_CHAT_DIR_LONGER_WIDER_OTF_dist8_slurm7180",
          "different_activations_orig_path": "bcywinski/gemma-2-9b-it-secret1",
          "layer": 30,
          "description": "Trained on WildChat dataset (ONLY)",
          "batch_size": 48,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma2-9b-chat-l10-secret",
          "name": "Gemma-2-9B-IT Chat L10 - AUDITING GAME",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_CHAT_9b_frozen_nopostfix_GG29SUS_gemma-2-9b-it_L10_e10_frozen_lr1e-4_t8_2ep_resume_0723_084137_ITBASE_NO_ENC_PROJ_NOW10_OTF_dist2_slurm6692",
          "different_activations_orig_path": "bcywinski/gemma-2-9b-it-secret1",
          "layer": 10,
          "description": "Openwebtext + Chat tuned, layer 10 finetuned from layer 30",
          "batch_size": 64,
          "auto_batch_size_max":  768,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma2-9b-chat-l20-secret",
          "name": "Gemma-2-9B-IT Chat L20 - AUDITING GAME",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_CHAT_9b_frozen_nopostfix_GG29SUS_gemma-2-9b-it_L20_e20_frozen_lr1e-4_t8_2ep_resume_0723_084137_ITBASE_NO_ENC_PROJ_NOW20_OTF_dist2_slurm6693",
          "different_activations_orig_path": "bcywinski/gemma-2-9b-it-secret1",
          "layer": 20,
          "description": "Openwebtext + Chat tuned, layer 20 finetuned from layer 30",
          "batch_size": 48,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma2-9b-chat-l30-secret",
          "name": "Gemma-2-9B-IT Chat L30 - AUDITING GAME",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_CHAT_9b_frozen_nopostfix_GG29SUS_gemma-2-9b-it_L30_e30_frozen_lr3e-4_t8_2ep_resume_0718_132614_ITBASE_NO_ENC_PROJ_OTF_dist4_slurm6570",
          "different_activations_orig_path": "bcywinski/gemma-2-9b-it-secret1",
          "layer": 30,
          "description": "Openwebtext + Chat tuned, layer 30",
          "batch_size": 48,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma2-9b-chat-l36-secret",
          "name": "Gemma-2-9B-IT Chat L36 - AUDITING GAME",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_CHAT_9b_frozen_nopostfix_GG29SUS_gemma-2-9b-it_L30_e36_frozen_lr1e-4_t8_2ep_resume_0718_131554_ITBASE_NO_ENC_PROJ_NOW36_OTF_dist2_slurm6561",
          "different_activations_orig_path": "bcywinski/gemma-2-9b-it-secret1",
          "layer": 36,
          "description": "Openwebtext + Chat tuned, layer 36 finetuned from layer 30",
          "batch_size": 48,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        },
        {
          "id": "gemma2-9b-chat-shallow5-secret",
          "name": "Gemma-2-9B-IT Chat Shallow Out layer 30 -> layer 5 - AUDITING GAME",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_CHAT_9b_frozen_nopostfix_GG29SUS_gemma-2-9b-it_L30_e5_frozen_lr1e-4_t8_2ep_resume_0718_132510_ITBASE_NO_ENC_PROJ_SHALLOWOUT_OTF_dist2_slurm6571",
          "different_activations_orig_path": "bcywinski/gemma-2-9b-it-secret1",
          "layer": 30,
          "description": "Openwebtext + Chat tuned, layer 30, output layer of the encoder is layer 5 (so very limited computation)",
          "batch_size": 64,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": false
        }
      ]
    },
    {
      "group_id": "gemma2-9b-it-auditing-secret-backend",
      "group_name": "Gemma 2 9B IT auditing backend",
      "base_model_path": "google/gemma-2-9b-it",
      "description": "Gemma 2 9B secret (42 layers) backend",
      "visible": true,
      "models": [
        {
          "id": "gemma2-9b-secret-l30-secret-backend",
          "name": "Gemma-2-9B-IT L30 - AUDITING - BACKEND",
          "lens_checkpoint_path": "/workspace/kitf/talkative-probes/talkative_autoencoder/outputs/checkpoints/gemma2_WILDCHAT_9b_frozen_nopostfix_GG29AW1S_gemma-2-9b-it_L30_e30_frozen_lr1e-4_t8_2ep_resume_0801_120554_NO_ENC_PROJ8_CHAT_DIR_LONGER_OTF_dist8_slurm6901",
          "different_activations_orig_path": "bcywinski/gemma-2-9b-it-secret1",
          "layer": 30,
          "description": "Trained autoencoder on chat dataset (ONLY), layer 30 (42 total)",
          "batch_size": 48,
          "auto_batch_size_max": 768,
          "visible": true,
          "backend_only": true 
        }
      ]
    }
  ],
  "settings": {
    "preload_groups": true,
    "default_group": "gemma3-27b-it",
    "max_cpu_models": 5,
    "use_bf16": true,
    "no_orig": true,
    "no_kl": true,
    "comparison_tl_checkpoint": false,
    "estimated_gpu_memory": {
      "gemma2-9b-it": "20GB",
      "gemma3-27b-it": "50GB",
      "qwen2.5-14b": "30GB",
      "qwen2.5-14b-EM": "55GB"
    }
  }
}